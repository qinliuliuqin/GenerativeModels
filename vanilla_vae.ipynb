{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vanilla_vae.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP4g8piLu6ty75+hyMQOkne",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qinliuliuqin/GenerativeModels/blob/main/vanilla_vae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# build datasets\n",
        "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, \n",
        "                                transform=transforms.ToTensor())\n",
        "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, \n",
        "                               transform=transforms.ToTensor())\n",
        "\n",
        "mb_size = 64\n",
        "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=mb_size, \n",
        "                                           shuffle=True, num_workers=2)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=mb_size, \n",
        "                                          shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "Z_dim = 100\n",
        "X_dim = 28\n",
        "Y_dim=28\n",
        "h_dim = 128\n",
        "c = 0\n",
        "lr = 1e-3\n",
        "\n",
        "\n",
        "def xavier_init(size):\n",
        "    in_dim = size[0]\n",
        "    xavier_stddev = 1. / np.sqrt(in_dim / 2.)\n",
        "    return torch.randn(*size) * xavier_stddev\n",
        "\n",
        "\n",
        "# =============================== Q(z|X) ======================================\n",
        "\n",
        "Wxh = xavier_init(size=[X_dim * Y_dim, h_dim])\n",
        "bxh = torch.zeros(h_dim)\n",
        "\n",
        "Whz_mu = xavier_init(size=[h_dim, Z_dim])\n",
        "bhz_mu = torch.zeros(Z_dim)\n",
        "\n",
        "Whz_var = xavier_init(size=[h_dim, Z_dim])\n",
        "bhz_var = torch.zeros(Z_dim)\n",
        "\n",
        "\n",
        "def Q(X):\n",
        "    X = X.flatten(start_dim=1)\n",
        "    h = nn.ReLU()(X @ Wxh + bxh.repeat(X.size(0), 1))\n",
        "    z_mu = h @ Whz_mu + bhz_mu.repeat(h.size(0), 1)\n",
        "    z_var = h @ Whz_var + bhz_var.repeat(h.size(0), 1)\n",
        "    return z_mu, z_var\n",
        "\n",
        "\n",
        "def sample_z(mu, log_var):\n",
        "    eps = torch.randn(mb_size, Z_dim)\n",
        "    return mu + torch.exp(log_var / 2) * eps\n",
        "\n",
        "\n",
        "# =============================== P(X|z) ======================================\n",
        "\n",
        "Wzh = xavier_init(size=[Z_dim, h_dim])\n",
        "bzh = torch.zeros(h_dim)\n",
        "\n",
        "Whx = xavier_init(size=[h_dim, X_dim * Y_dim])\n",
        "bhx = torch.zeros(X_dim * Y_dim)\n",
        "\n",
        "def P(z):\n",
        "    h = nn.ReLU()(z @ Wzh + bzh.repeat(z.size(0), 1))\n",
        "    X = nn.Sigmoid()(h @ Whx + bhx.repeat(h.size(0), 1))\n",
        "\n",
        "    X = X.reshape(-1, X_dim, Y_dim)\n",
        "    return X\n",
        "\n",
        "\n",
        "# =============================== TRAINING ====================================\n",
        "\n",
        "params = [Wxh, bxh, Whz_mu, bhz_mu, Whz_var, bhz_var,\n",
        "          Wzh, bzh, Whx, bhx]\n",
        "\n",
        "for param in params:\n",
        "  param.requires_grad = True\n",
        "\n",
        "solver = optim.Adam(params, lr=lr)\n",
        "\n",
        "for it in range(500):\n",
        "    X, _ = next(iter(train_loader))\n",
        "    X = X.squeeze(dim=1)\n",
        "\n",
        "    # Forward\n",
        "    z_mu, z_var = Q(X)\n",
        "    z = sample_z(z_mu, z_var)\n",
        "    X_sample = P(z)\n",
        "\n",
        "    # Loss\n",
        "    recon_loss = F.binary_cross_entropy(X_sample, X, reduction='sum') / mb_size\n",
        "    kl_loss = torch.mean(0.5 * torch.sum(torch.exp(z_var) + z_mu**2 - 1. - z_var, 1))\n",
        "    loss = recon_loss + kl_loss\n",
        "\n",
        "    # Backward\n",
        "    loss.backward()\n",
        "\n",
        "    # Update\n",
        "    solver.step()\n",
        "\n",
        "    # Housekeeping\n",
        "    for p in params:\n",
        "        if p.grad is not None:\n",
        "            data = p.grad.data\n",
        "            p.grad = data.new().resize_as_(data).zero_()\n",
        "\n",
        "    # Print and plot every now and then\n",
        "    if it % 10 == 0:\n",
        "        print('Iter-{}; Loss: {:.4}'.format(it, loss.item()))\n",
        "\n",
        "        samples = P(z).data.numpy()[:16]\n",
        "\n",
        "        fig = plt.figure(figsize=(4, 4))\n",
        "        gs = gridspec.GridSpec(4, 4)\n",
        "        gs.update(wspace=0.05, hspace=0.05)\n",
        "\n",
        "        for i, sample in enumerate(samples):\n",
        "            ax = plt.subplot(gs[i])\n",
        "            plt.axis('off')\n",
        "            ax.set_xticklabels([])\n",
        "            ax.set_yticklabels([])\n",
        "            ax.set_aspect('equal')\n",
        "            plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
        "\n",
        "        if not os.path.exists('out/'):\n",
        "            os.makedirs('out/')\n",
        "\n",
        "        plt.savefig('out/{}.png'.format(str(c).zfill(3)), bbox_inches='tight')\n",
        "        c += 1\n",
        "        plt.close(fig)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pd3YYlfVJTD",
        "outputId": "e665cc8d-2251-4aea-fd0b-fcefdc430616"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter-0; Loss: 786.9\n",
            "Iter-10; Loss: 535.3\n",
            "Iter-20; Loss: 424.2\n",
            "Iter-30; Loss: 339.9\n",
            "Iter-40; Loss: 289.2\n",
            "Iter-50; Loss: 271.2\n",
            "Iter-60; Loss: 254.3\n",
            "Iter-70; Loss: 234.8\n",
            "Iter-80; Loss: 238.7\n",
            "Iter-90; Loss: 223.8\n",
            "Iter-100; Loss: 234.4\n",
            "Iter-110; Loss: 230.9\n",
            "Iter-120; Loss: 221.7\n",
            "Iter-130; Loss: 212.3\n",
            "Iter-140; Loss: 208.5\n",
            "Iter-150; Loss: 207.8\n",
            "Iter-160; Loss: 206.9\n",
            "Iter-170; Loss: 207.7\n",
            "Iter-180; Loss: 202.9\n",
            "Iter-190; Loss: 203.8\n",
            "Iter-200; Loss: 193.9\n",
            "Iter-210; Loss: 178.4\n",
            "Iter-220; Loss: 186.2\n",
            "Iter-230; Loss: 193.2\n",
            "Iter-240; Loss: 188.3\n",
            "Iter-250; Loss: 187.2\n",
            "Iter-260; Loss: 187.1\n",
            "Iter-270; Loss: 188.9\n",
            "Iter-280; Loss: 174.3\n",
            "Iter-290; Loss: 179.8\n",
            "Iter-300; Loss: 181.1\n",
            "Iter-310; Loss: 177.2\n",
            "Iter-320; Loss: 178.5\n",
            "Iter-330; Loss: 174.4\n",
            "Iter-340; Loss: 175.8\n",
            "Iter-350; Loss: 176.0\n",
            "Iter-360; Loss: 179.3\n",
            "Iter-370; Loss: 168.8\n",
            "Iter-380; Loss: 174.8\n",
            "Iter-390; Loss: 171.1\n",
            "Iter-400; Loss: 156.7\n",
            "Iter-410; Loss: 174.4\n",
            "Iter-420; Loss: 170.8\n",
            "Iter-430; Loss: 172.1\n",
            "Iter-440; Loss: 169.1\n",
            "Iter-450; Loss: 166.2\n",
            "Iter-460; Loss: 169.9\n",
            "Iter-470; Loss: 164.3\n",
            "Iter-480; Loss: 170.9\n",
            "Iter-490; Loss: 164.1\n"
          ]
        }
      ]
    }
  ]
}