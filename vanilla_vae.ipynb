{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOw0MAiM/ZFERUPHAUHtOrd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qinliuliuqin/GenerativeModels/blob/main/vanilla_vae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# build datasets\n",
        "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, \n",
        "                                transform=transforms.ToTensor())\n",
        "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, \n",
        "                               transform=transforms.ToTensor())\n",
        "\n",
        "mb_size = 64\n",
        "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=mb_size, \n",
        "                                           shuffle=True, num_workers=2)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=mb_size, \n",
        "                                          shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "Z_dim = 100\n",
        "X_dim = 28\n",
        "Y_dim=28\n",
        "h_dim = 128\n",
        "c = 0\n",
        "lr = 1e-3\n",
        "\n",
        "\n",
        "def xavier_init(size):\n",
        "    in_dim = size[0]\n",
        "    xavier_stddev = 1. / np.sqrt(in_dim / 2.)\n",
        "    return torch.randn(*size) * xavier_stddev\n",
        "\n",
        "\n",
        "# =============================== Q(z|X) ======================================\n",
        "\n",
        "Wxh = xavier_init(size=[X_dim * Y_dim, h_dim])\n",
        "bxh = torch.zeros(h_dim)\n",
        "\n",
        "Whz_mu = xavier_init(size=[h_dim, Z_dim])\n",
        "bhz_mu = torch.zeros(Z_dim)\n",
        "\n",
        "Whz_var = xavier_init(size=[h_dim, Z_dim])\n",
        "bhz_var = torch.zeros(Z_dim)\n",
        "\n",
        "\n",
        "def Q(X):\n",
        "    X = X.flatten(start_dim=1)\n",
        "    h = nn.ReLU()(X @ Wxh + bxh.repeat(X.size(0), 1))\n",
        "    z_mu = h @ Whz_mu + bhz_mu.repeat(h.size(0), 1)\n",
        "    z_var = h @ Whz_var + bhz_var.repeat(h.size(0), 1)\n",
        "    return z_mu, z_var\n",
        "\n",
        "\n",
        "def sample_z(mu, log_var):\n",
        "    eps = torch.randn(mb_size, Z_dim)\n",
        "    return mu + torch.exp(log_var / 2) * eps\n",
        "\n",
        "\n",
        "# =============================== P(X|z) ======================================\n",
        "\n",
        "Wzh = xavier_init(size=[Z_dim, h_dim])\n",
        "bzh = torch.zeros(h_dim)\n",
        "\n",
        "Whx = xavier_init(size=[h_dim, X_dim * Y_dim])\n",
        "bhx = torch.zeros(X_dim * Y_dim)\n",
        "\n",
        "def P(z):\n",
        "    h = nn.ReLU()(z @ Wzh + bzh.repeat(z.size(0), 1))\n",
        "    X = nn.Sigmoid()(h @ Whx + bhx.repeat(h.size(0), 1))\n",
        "\n",
        "    X = X.reshape(-1, X_dim, Y_dim)\n",
        "    return X\n",
        "\n",
        "\n",
        "# =============================== TRAINING ====================================\n",
        "\n",
        "params = [Wxh, bxh, Whz_mu, bhz_mu, Whz_var, bhz_var,\n",
        "          Wzh, bzh, Whx, bhx]\n",
        "\n",
        "for param in params:\n",
        "  param.requires_grad = True\n",
        "\n",
        "solver = optim.Adam(params, lr=lr)\n",
        "\n",
        "for it in range(1000):\n",
        "    X, _ = next(iter(train_loader))\n",
        "    X = X.squeeze(dim=1)\n",
        "\n",
        "    # Forward\n",
        "    z_mu, z_var = Q(X)\n",
        "    z = sample_z(z_mu, z_var)\n",
        "    X_sample = P(z)\n",
        "\n",
        "    # Loss\n",
        "    recon_loss = F.binary_cross_entropy(X_sample, X, size_average=False) / mb_size\n",
        "    kl_loss = torch.mean(0.5 * torch.sum(torch.exp(z_var) + z_mu**2 - 1. - z_var, 1))\n",
        "    loss = recon_loss + kl_loss\n",
        "\n",
        "    # Backward\n",
        "    loss.backward()\n",
        "\n",
        "    # Update\n",
        "    solver.step()\n",
        "\n",
        "    # Housekeeping\n",
        "    for p in params:\n",
        "        if p.grad is not None:\n",
        "            data = p.grad.data\n",
        "            p.grad = data.new().resize_as_(data).zero_()\n",
        "\n",
        "    # Print and plot every now and then\n",
        "    if it % 10 == 0:\n",
        "        print('Iter-{}; Loss: {:.4}'.format(it, loss.item()))\n",
        "\n",
        "        samples = P(z).data.numpy()[:16]\n",
        "\n",
        "        fig = plt.figure(figsize=(4, 4))\n",
        "        gs = gridspec.GridSpec(4, 4)\n",
        "        gs.update(wspace=0.05, hspace=0.05)\n",
        "\n",
        "        for i, sample in enumerate(samples):\n",
        "            ax = plt.subplot(gs[i])\n",
        "            plt.axis('off')\n",
        "            ax.set_xticklabels([])\n",
        "            ax.set_yticklabels([])\n",
        "            ax.set_aspect('equal')\n",
        "            plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
        "\n",
        "        if not os.path.exists('out/'):\n",
        "            os.makedirs('out/')\n",
        "\n",
        "        plt.savefig('out/{}.png'.format(str(c).zfill(3)), bbox_inches='tight')\n",
        "        c += 1\n",
        "        plt.close(fig)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pd3YYlfVJTD",
        "outputId": "4402530e-9114-4fd7-d492-4c1957b02d04"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter-0; Loss: 802.9\n",
            "Iter-10; Loss: 524.0\n",
            "Iter-20; Loss: 420.7\n",
            "Iter-30; Loss: 329.3\n",
            "Iter-40; Loss: 264.9\n",
            "Iter-50; Loss: 257.8\n",
            "Iter-60; Loss: 255.3\n",
            "Iter-70; Loss: 241.6\n",
            "Iter-80; Loss: 240.0\n",
            "Iter-90; Loss: 235.4\n",
            "Iter-100; Loss: 226.1\n",
            "Iter-110; Loss: 235.2\n",
            "Iter-120; Loss: 221.7\n",
            "Iter-130; Loss: 215.3\n",
            "Iter-140; Loss: 216.4\n",
            "Iter-150; Loss: 211.7\n",
            "Iter-160; Loss: 218.2\n",
            "Iter-170; Loss: 206.6\n",
            "Iter-180; Loss: 202.9\n",
            "Iter-190; Loss: 206.3\n",
            "Iter-200; Loss: 198.6\n",
            "Iter-210; Loss: 196.2\n",
            "Iter-220; Loss: 179.7\n",
            "Iter-230; Loss: 183.8\n",
            "Iter-240; Loss: 191.4\n",
            "Iter-250; Loss: 185.4\n",
            "Iter-260; Loss: 182.2\n",
            "Iter-270; Loss: 182.0\n",
            "Iter-280; Loss: 177.9\n",
            "Iter-290; Loss: 183.6\n",
            "Iter-300; Loss: 187.0\n",
            "Iter-310; Loss: 167.9\n",
            "Iter-320; Loss: 173.8\n",
            "Iter-330; Loss: 167.3\n",
            "Iter-340; Loss: 170.9\n",
            "Iter-350; Loss: 173.7\n",
            "Iter-360; Loss: 179.0\n",
            "Iter-370; Loss: 182.5\n",
            "Iter-380; Loss: 168.4\n",
            "Iter-390; Loss: 175.3\n",
            "Iter-400; Loss: 170.4\n",
            "Iter-410; Loss: 169.8\n",
            "Iter-420; Loss: 160.1\n",
            "Iter-430; Loss: 162.9\n",
            "Iter-440; Loss: 169.7\n",
            "Iter-450; Loss: 173.7\n",
            "Iter-460; Loss: 171.9\n",
            "Iter-470; Loss: 163.2\n",
            "Iter-480; Loss: 162.8\n",
            "Iter-490; Loss: 163.6\n",
            "Iter-500; Loss: 164.6\n",
            "Iter-510; Loss: 169.1\n",
            "Iter-520; Loss: 160.5\n",
            "Iter-530; Loss: 171.5\n",
            "Iter-540; Loss: 154.6\n",
            "Iter-550; Loss: 154.7\n",
            "Iter-560; Loss: 147.5\n",
            "Iter-570; Loss: 157.8\n",
            "Iter-580; Loss: 159.6\n",
            "Iter-590; Loss: 167.9\n",
            "Iter-600; Loss: 164.7\n",
            "Iter-610; Loss: 157.0\n",
            "Iter-620; Loss: 156.5\n",
            "Iter-630; Loss: 158.3\n",
            "Iter-640; Loss: 152.4\n",
            "Iter-650; Loss: 160.4\n",
            "Iter-660; Loss: 156.2\n",
            "Iter-670; Loss: 153.9\n",
            "Iter-680; Loss: 157.5\n",
            "Iter-690; Loss: 150.9\n",
            "Iter-700; Loss: 153.7\n",
            "Iter-710; Loss: 142.3\n",
            "Iter-720; Loss: 148.2\n",
            "Iter-730; Loss: 154.0\n",
            "Iter-740; Loss: 158.9\n",
            "Iter-750; Loss: 156.5\n",
            "Iter-760; Loss: 155.5\n",
            "Iter-770; Loss: 153.5\n",
            "Iter-780; Loss: 150.1\n",
            "Iter-790; Loss: 149.6\n",
            "Iter-800; Loss: 150.7\n",
            "Iter-810; Loss: 156.2\n",
            "Iter-820; Loss: 151.5\n",
            "Iter-830; Loss: 153.9\n",
            "Iter-840; Loss: 146.4\n",
            "Iter-850; Loss: 146.0\n",
            "Iter-860; Loss: 155.8\n",
            "Iter-870; Loss: 154.2\n",
            "Iter-880; Loss: 151.8\n",
            "Iter-890; Loss: 158.9\n",
            "Iter-900; Loss: 147.8\n",
            "Iter-910; Loss: 143.7\n",
            "Iter-920; Loss: 153.6\n",
            "Iter-930; Loss: 151.5\n",
            "Iter-940; Loss: 147.4\n",
            "Iter-950; Loss: 154.5\n",
            "Iter-960; Loss: 146.1\n",
            "Iter-970; Loss: 143.0\n",
            "Iter-980; Loss: 152.6\n",
            "Iter-990; Loss: 148.8\n"
          ]
        }
      ]
    }
  ]
}