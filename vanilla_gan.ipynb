{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vanilla_gan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMIyn6etw72UvauV7Keo1R6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qinliuliuqin/GenerativeModels/blob/main/vanilla_gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLG3riXq0C4p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a8b722c-9075-4a8e-dc97-bba2b6d172ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter-0; D_loss: 1.7111154794692993; G_loss: 2.3246514797210693\n",
            "Iter-100; D_loss: 0.24809274077415466; G_loss: 3.9886956214904785\n",
            "Iter-200; D_loss: 0.05050894245505333; G_loss: 5.8243608474731445\n",
            "Iter-300; D_loss: 0.07821844518184662; G_loss: 5.957636833190918\n",
            "Iter-400; D_loss: 0.02358751744031906; G_loss: 5.9996185302734375\n",
            "Iter-500; D_loss: 0.011351916939020157; G_loss: 6.457283973693848\n",
            "Iter-600; D_loss: 0.004365156404674053; G_loss: 7.674680709838867\n",
            "Iter-700; D_loss: 0.020695002749562263; G_loss: 9.446849822998047\n",
            "Iter-800; D_loss: 0.008205423131585121; G_loss: 7.852683067321777\n",
            "Iter-900; D_loss: 0.0062996563501656055; G_loss: 7.8452630043029785\n",
            "Iter-1000; D_loss: 0.0038198246620595455; G_loss: 8.367925643920898\n",
            "Iter-1100; D_loss: 0.004194531124085188; G_loss: 8.40261173248291\n",
            "Iter-1200; D_loss: 0.0043449099175632; G_loss: 7.920822620391846\n",
            "Iter-1300; D_loss: 0.0031598899513483047; G_loss: 7.968366622924805\n",
            "Iter-1400; D_loss: 0.006255622021853924; G_loss: 8.467569351196289\n",
            "Iter-1500; D_loss: 0.00762838963419199; G_loss: 7.347851753234863\n",
            "Iter-1600; D_loss: 0.011079504154622555; G_loss: 7.731291770935059\n",
            "Iter-1700; D_loss: 0.0023659018334001303; G_loss: 10.067119598388672\n",
            "Iter-1800; D_loss: 0.008863778784871101; G_loss: 9.450980186462402\n",
            "Iter-1900; D_loss: 0.016621984541416168; G_loss: 6.85522985458374\n",
            "Iter-2000; D_loss: 0.008808189071714878; G_loss: 7.840385913848877\n",
            "Iter-2100; D_loss: 0.014940280467271805; G_loss: 7.5507354736328125\n",
            "Iter-2200; D_loss: 0.009594113565981388; G_loss: 7.201211929321289\n",
            "Iter-2300; D_loss: 0.007038326933979988; G_loss: 8.789590835571289\n",
            "Iter-2400; D_loss: 0.007800644263625145; G_loss: 8.670849800109863\n",
            "Iter-2500; D_loss: 0.005311096087098122; G_loss: 9.519367218017578\n",
            "Iter-2600; D_loss: 0.01065459381788969; G_loss: 7.225538730621338\n",
            "Iter-2700; D_loss: 0.020216304808855057; G_loss: 6.395639419555664\n",
            "Iter-2800; D_loss: 0.05092446878552437; G_loss: 5.502010822296143\n",
            "Iter-2900; D_loss: 0.04580400511622429; G_loss: 6.273608684539795\n",
            "Iter-3000; D_loss: 0.03424500301480293; G_loss: 5.666315078735352\n",
            "Iter-3100; D_loss: 0.03070509247481823; G_loss: 5.783835411071777\n",
            "Iter-3200; D_loss: 0.04344059154391289; G_loss: 6.037167549133301\n",
            "Iter-3300; D_loss: 0.03441224247217178; G_loss: 6.220184326171875\n",
            "Iter-3400; D_loss: 0.07514561712741852; G_loss: 6.035269260406494\n",
            "Iter-3500; D_loss: 0.0289594903588295; G_loss: 4.84950590133667\n",
            "Iter-3600; D_loss: 0.011669822968542576; G_loss: 7.102438926696777\n",
            "Iter-3700; D_loss: 0.07420524954795837; G_loss: 5.608421325683594\n",
            "Iter-3800; D_loss: 0.09473676234483719; G_loss: 7.838368892669678\n",
            "Iter-3900; D_loss: 0.03036179021000862; G_loss: 7.599388122558594\n",
            "Iter-4000; D_loss: 0.06147458404302597; G_loss: 6.620971202850342\n",
            "Iter-4100; D_loss: 0.030029600486159325; G_loss: 7.066598415374756\n",
            "Iter-4200; D_loss: 0.17614182829856873; G_loss: 6.109859466552734\n",
            "Iter-4300; D_loss: 0.07438529282808304; G_loss: 5.095487117767334\n",
            "Iter-4400; D_loss: 0.07613673061132431; G_loss: 5.326683044433594\n",
            "Iter-4500; D_loss: 0.07659453898668289; G_loss: 5.068537712097168\n",
            "Iter-4600; D_loss: 0.12371572107076645; G_loss: 5.071019649505615\n",
            "Iter-4700; D_loss: 0.22606752812862396; G_loss: 4.952014923095703\n",
            "Iter-4800; D_loss: 0.2966994643211365; G_loss: 4.4420671463012695\n",
            "Iter-4900; D_loss: 0.21417438983917236; G_loss: 5.890035629272461\n",
            "Iter-5000; D_loss: 0.0987502783536911; G_loss: 5.985382080078125\n",
            "Iter-5100; D_loss: 0.29848921298980713; G_loss: 4.544315814971924\n",
            "Iter-5200; D_loss: 0.4329618215560913; G_loss: 4.330350875854492\n",
            "Iter-5300; D_loss: 0.3834264278411865; G_loss: 3.4966843128204346\n",
            "Iter-5400; D_loss: 0.12393695116043091; G_loss: 4.731273651123047\n",
            "Iter-5500; D_loss: 0.4484187960624695; G_loss: 5.178558349609375\n",
            "Iter-5600; D_loss: 0.159557044506073; G_loss: 4.534686088562012\n",
            "Iter-5700; D_loss: 0.29879510402679443; G_loss: 4.431934356689453\n",
            "Iter-5800; D_loss: 0.2200113981962204; G_loss: 3.918576717376709\n",
            "Iter-5900; D_loss: 0.37897932529449463; G_loss: 4.047003269195557\n",
            "Iter-6000; D_loss: 0.4039749503135681; G_loss: 3.058011054992676\n",
            "Iter-6100; D_loss: 0.6195545196533203; G_loss: 3.1544556617736816\n",
            "Iter-6200; D_loss: 0.2945946753025055; G_loss: 4.488465785980225\n",
            "Iter-6300; D_loss: 0.29449981451034546; G_loss: 3.874105215072632\n",
            "Iter-6400; D_loss: 0.6099502444267273; G_loss: 4.155328750610352\n",
            "Iter-6500; D_loss: 0.3969647288322449; G_loss: 3.094900131225586\n",
            "Iter-6600; D_loss: 0.6419116258621216; G_loss: 2.742546319961548\n",
            "Iter-6700; D_loss: 0.43232619762420654; G_loss: 3.2627079486846924\n",
            "Iter-6800; D_loss: 0.6961616277694702; G_loss: 3.6873397827148438\n",
            "Iter-6900; D_loss: 0.30300503969192505; G_loss: 3.3077735900878906\n",
            "Iter-7000; D_loss: 0.33422935009002686; G_loss: 3.194847583770752\n",
            "Iter-7100; D_loss: 0.355281800031662; G_loss: 3.942708969116211\n",
            "Iter-7200; D_loss: 0.34979915618896484; G_loss: 2.98382830619812\n",
            "Iter-7300; D_loss: 0.3394899368286133; G_loss: 3.1688153743743896\n",
            "Iter-7400; D_loss: 0.3306732475757599; G_loss: 3.617604970932007\n",
            "Iter-7500; D_loss: 0.599595308303833; G_loss: 2.589223623275757\n",
            "Iter-7600; D_loss: 0.5411719083786011; G_loss: 3.311164140701294\n",
            "Iter-7700; D_loss: 0.41779419779777527; G_loss: 3.8399806022644043\n",
            "Iter-7800; D_loss: 0.46033182740211487; G_loss: 3.6844229698181152\n",
            "Iter-7900; D_loss: 0.31447872519493103; G_loss: 2.763948440551758\n",
            "Iter-8000; D_loss: 0.5713738203048706; G_loss: 2.7778313159942627\n",
            "Iter-8100; D_loss: 0.44075682759284973; G_loss: 3.1673941612243652\n",
            "Iter-8200; D_loss: 0.3788248896598816; G_loss: 2.63948655128479\n",
            "Iter-8300; D_loss: 0.5958965420722961; G_loss: 2.727081775665283\n",
            "Iter-8400; D_loss: 0.31007999181747437; G_loss: 3.419492483139038\n",
            "Iter-8500; D_loss: 0.7655053734779358; G_loss: 2.9387636184692383\n",
            "Iter-8600; D_loss: 0.4806930422782898; G_loss: 2.488823175430298\n",
            "Iter-8700; D_loss: 0.6234334707260132; G_loss: 2.7764859199523926\n",
            "Iter-8800; D_loss: 0.4632161259651184; G_loss: 2.8640949726104736\n",
            "Iter-8900; D_loss: 0.4270292818546295; G_loss: 3.094974994659424\n",
            "Iter-9000; D_loss: 0.3516387939453125; G_loss: 3.164306163787842\n",
            "Iter-9100; D_loss: 0.5009187459945679; G_loss: 2.294184684753418\n",
            "Iter-9200; D_loss: 0.4358745217323303; G_loss: 2.839883327484131\n",
            "Iter-9300; D_loss: 0.7897019386291504; G_loss: 2.6744368076324463\n",
            "Iter-9400; D_loss: 0.6545987129211426; G_loss: 2.996034860610962\n",
            "Iter-9500; D_loss: 0.43194496631622314; G_loss: 2.9620349407196045\n",
            "Iter-9600; D_loss: 0.90165114402771; G_loss: 2.6071934700012207\n",
            "Iter-9700; D_loss: 0.5455179214477539; G_loss: 3.118468761444092\n",
            "Iter-9800; D_loss: 0.4300556480884552; G_loss: 2.4315404891967773\n",
            "Iter-9900; D_loss: 0.37226057052612305; G_loss: 3.20811128616333\n",
            "Iter-10000; D_loss: 0.5767422914505005; G_loss: 2.8892712593078613\n",
            "Iter-10100; D_loss: 0.6142292618751526; G_loss: 2.809265613555908\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# build datasets\n",
        "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, \n",
        "                                transform=transforms.ToTensor())\n",
        "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, \n",
        "                               transform=transforms.ToTensor())\n",
        "\n",
        "mb_size = 64\n",
        "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=mb_size, \n",
        "                                           shuffle=True, num_workers=2)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=mb_size, \n",
        "                                          shuffle=False, num_workers=2)\n",
        "\n",
        "Z_dim = 100\n",
        "X_dim = 28\n",
        "Y_dim=28\n",
        "h_dim = 128\n",
        "c = 0\n",
        "lr = 1e-3\n",
        "\n",
        "def xavier_init(size):\n",
        "    in_dim = size[0]\n",
        "    xavier_stddev = 1. / np.sqrt(in_dim / 2.)\n",
        "    return torch.randn(*size) * xavier_stddev\n",
        "\n",
        "\n",
        "\"\"\" ==================== GENERATOR ======================== \"\"\"\n",
        "\n",
        "Wzh = xavier_init(size=[Z_dim, h_dim])\n",
        "bzh = torch.zeros(h_dim)\n",
        "\n",
        "Whx = xavier_init(size=[h_dim, X_dim * Y_dim])\n",
        "bhx = torch.zeros(X_dim * Y_dim)\n",
        "\n",
        "def G(z):\n",
        "    h = nn.ReLU()(z @ Wzh + bzh.repeat(z.size(0), 1))\n",
        "    X = nn.Sigmoid()(h @ Whx + bhx.repeat(h.size(0), 1))\n",
        "    return X\n",
        "\n",
        "\n",
        "\"\"\" ==================== DISCRIMINATOR ======================== \"\"\"\n",
        "\n",
        "Wxh = xavier_init(size=[X_dim * Y_dim, h_dim])\n",
        "bxh = torch.zeros(h_dim)\n",
        "\n",
        "Why = xavier_init(size=[h_dim, 1])\n",
        "bhy = torch.zeros(1)\n",
        "\n",
        "def D(X):\n",
        "    X = X.flatten(start_dim=1)\n",
        "    h = nn.ReLU()(X @ Wxh + bxh.repeat(X.size(0), 1))\n",
        "    y = nn.Sigmoid()(h @ Why + bhy.repeat(h.size(0), 1))\n",
        "    return y\n",
        "\n",
        "G_params = [Wzh, bzh, Whx, bhx]\n",
        "D_params = [Wxh, bxh, Why, bhy]\n",
        "params = G_params + D_params\n",
        "\n",
        "for param in params:\n",
        "  param.requires_grad = True\n",
        "\n",
        "\n",
        "\"\"\" ===================== TRAINING ======================== \"\"\"\n",
        "\n",
        "\n",
        "def reset_grad():\n",
        "    for p in params:\n",
        "        if p.grad is not None:\n",
        "            data = p.grad.data\n",
        "            p.grad = data.new().resize_as_(data).zero_()\n",
        "\n",
        "\n",
        "G_solver = optim.Adam(G_params, lr=1e-3)\n",
        "D_solver = optim.Adam(D_params, lr=1e-3)\n",
        "\n",
        "ones_label = torch.ones(mb_size, 1)\n",
        "zeros_label = torch.zeros(mb_size, 1)\n",
        "\n",
        "\n",
        "for it in range(100000):\n",
        "    # Sample data\n",
        "    z = torch.randn(mb_size, Z_dim)\n",
        "    X, _ = next(iter(train_loader))\n",
        "    X = X.squeeze(dim=1)\n",
        "\n",
        "    # Dicriminator forward-loss-backward-update\n",
        "    for param in G_params:\n",
        "      param.requires_grad = False\n",
        "    for param in D_params:\n",
        "      param.requires_grad = True\n",
        "\n",
        "    G_sample = G(z)\n",
        "    D_real = D(X)\n",
        "    D_fake = D(G_sample)\n",
        "\n",
        "    D_loss_real = F.binary_cross_entropy(D_real, ones_label)\n",
        "    D_loss_fake = F.binary_cross_entropy(D_fake, zeros_label)\n",
        "    D_loss = D_loss_real + D_loss_fake\n",
        "\n",
        "    D_loss.backward()\n",
        "    D_solver.step()\n",
        "\n",
        "    # Housekeeping - reset gradient\n",
        "    reset_grad()\n",
        "\n",
        "    # Generator forward-loss-backward-update\n",
        "    for param in G_params:\n",
        "      param.requires_grad = True\n",
        "    for param in D_params:\n",
        "      param.requires_grad = False\n",
        "\n",
        "    z = torch.randn(mb_size, Z_dim)\n",
        "    G_sample = G(z)\n",
        "    D_fake = D(G_sample)\n",
        "\n",
        "    G_loss = F.binary_cross_entropy(D_fake, ones_label)\n",
        "\n",
        "    G_loss.backward()\n",
        "    G_solver.step()\n",
        "\n",
        "    # Housekeeping - reset gradient\n",
        "    reset_grad()\n",
        "\n",
        "    # Print and plot every now and then\n",
        "    if it % 100 == 0:\n",
        "        print('Iter-{}; D_loss: {}; G_loss: {}'.format(it, D_loss.data.numpy(), \n",
        "                                                       G_loss.data.numpy()))\n",
        "\n",
        "        samples = G(z).data.numpy()[:16]\n",
        "\n",
        "        fig = plt.figure(figsize=(4, 4))\n",
        "        gs = gridspec.GridSpec(4, 4)\n",
        "        gs.update(wspace=0.05, hspace=0.05)\n",
        "\n",
        "        for i, sample in enumerate(samples):\n",
        "            ax = plt.subplot(gs[i])\n",
        "            plt.axis('off')\n",
        "            ax.set_xticklabels([])\n",
        "            ax.set_yticklabels([])\n",
        "            ax.set_aspect('equal')\n",
        "            plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
        "\n",
        "        if not os.path.exists('out/'):\n",
        "            os.makedirs('out/')\n",
        "\n",
        "        plt.savefig('out/{}.png'.format(str(c).zfill(3)), bbox_inches='tight')\n",
        "        c += 1\n",
        "        plt.close(fig)"
      ]
    }
  ]
}